* We are prone to overestimate how much we understand about the world and to underestimate the role of chance in events. Overconfidence is fed by the illusory certainty of hindsight... [We must] intelligently explore the lessons that can be learned from the past while resisting the lure of hindsight and the illusion of certainty.

* [Daniel Kahneman: Thinking Fast and Slow, Deep Learning, and AI | Lex Fridman Podcast #65](https://youtu.be/UwwBG-MbniY)
    > "The 12 or 13 years in which most of our work was joint were years of interpersonal and intellectual bliss. Everything was interesting. Almost everything was funny. And there was a current joy of seeing an idea take shape. So many times in those years we shared the magical experience of one of us saying something of which the other one would understand more deeply than the speaker had done. Contrary to the old laws of information theory, it was common for us to find that more information had been received than had been sent. I've not had this experience with anyone else. If you've not had it, you don't know how marvellous collaboration can be..." - Daniel Kahnemann

    > There is a difference worth noting in Yann LeCun's approach (who Prof. Kahneman mentioned)  and Yoshua Bengio's approach to solve System 2 Deep learning tasks (at least from what we can glean from their recent talks).
    While both are focussing on self-supervised learning (learning by predicting/reconstructing missing parts of input), Yann LeCunn's approach is for models to learn by predicting what happens next in input space. Yoshua Bengio's approach is to learn by predicting what happens next in an abstract space - not directly in input space. The input space and abstract space maps to what Prof. Kahneman refers to in the middle of your conversation as "experience" (input space) and "memory" (abstract space). Prof. Kahneman even elaborates that our memory of experiences is not a full replay of experience but a compressed version(low dimensional version in ML speak) of it.
    Yoshua's approach is to make prediction in that abstract space and learn from that. When we see a person let go of a pen - we predict it will fall, not the exact position it will fall. When we plan our trip to a place, we plan not the actual experience of the trip but the salient aspects of it. Making prediction in a low dimensional (System 2 representations) abstract space that is anchored in representations learned from perception (System 1 representations)  is based on the assumption that changes in the world can be explained by a few causal variables - making predictions in such a space helps the model learn representations that capture causality, which system 1 Deep learning lacks. If this assumption is true, then predicting what happens in abstract space has the benefit of learning causal variables that are invariant to underlying changes in distribution in the input space. The training objective for predicting in the abstract space leverages off the changes in the underlying input space distribution as the means to learn its representation and its prediction performance serves as a metric to evaluate its learned representations. However, there are challenges to predicting in the abstract low dimensional space  - specifically what would the training objective exactly be ..."
